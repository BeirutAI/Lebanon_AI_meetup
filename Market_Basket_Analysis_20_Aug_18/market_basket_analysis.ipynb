{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='float:right;border-radius:5%;width:20%;height:20%;background-blend-mode:screen;margin-right:25px;margin-top:25px;' src='mirum_purple.png'>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<h1 style='text-align:center;'> Market Basket Analysis </h1>\n",
    "<br>\n",
    "\n",
    "**Instructor**: Elie Kawerk, Ph.D. \n",
    "<p style=\"padding-left:74px\"> Data Scientist at Mirum Agency MEA </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='float:right;width:250px;height:210px;margin-right:30px;padding-left:15px;' src='http://1.bp.blogspot.com/-sYQ4ex-tM0E/UnKYjSpV5cI/AAAAAAAABLY/ZXWnHiADtyw/s1600/social-media-humor-21.png'>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The goal of market basket analysis is to discover items that are most likely to be purchased together. Such discoveries are usually done by mining historical data of transactions. This discipline is very important for e-commerce websites and retail magazines as it allows them to better organize their layouts. \n",
    "\n",
    "In this workshop, you will use the customers' purchase history of a grocery store to cluster items that should be placed or bundled together.\n",
    "\n",
    "The datasets are available in this repo as `csv` files:\n",
    "\n",
    "- `purchase_history.csv`: contains the data of individual baskets or carts. It consists of 2 columns, the first column `customer_id` refers to the id of a customer, while the 2nd column `basket` refers to the item ids purchased by the customer. Note that the item ids are separated by commas.\n",
    "<br>\n",
    "<br>\n",
    "- `item_to_id.csv`: contains the mapping between the names of the grocery items (`Item_name`) and their corresponding ids (`Item_id`).\n",
    "\n",
    "\n",
    "### API Versions\n",
    "\n",
    "You can first start by checking the versions of the libraries that you'll be using throughout the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the watermark extension\n",
    "# If watermark is not available, go to your terminal and type: pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your name here\n",
    "%watermark  -a \"Author: Your name here; Date: \" -dt -v -p sklearn,pandas,matplotlib,seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis and Plotting\n",
    "import ____ as ____  # pandas  as pd\n",
    "import ____.____ as ____ # pyplot as plt\n",
    "import numpy as np # numpy\n",
    "import ____ as ____ # seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D # 3D plotting\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "# Other utilities\n",
    "import itertools as it\n",
    "from collections import Counter\n",
    "\n",
    "# display\n",
    "from IPython.display import display\n",
    "\n",
    "# Set custom preferences for displaying and visualizing data\n",
    "sns.set_style('white')\n",
    "pd.set_option('max_colwidth', 120)\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import and Inspect the Datasets\n",
    "\n",
    "You can start by reading the two `csv` files containing the datasets.\n",
    "\n",
    "**Task**: Use `pd.read_csv` to read the two available datasets as `pandas.DataFrame`s. \n",
    "\n",
    "- Assign the dataframe corresponding to `purchase_history.csv` to `df_baskets`.\n",
    "- Assign the dataframe corresponding to `item_to_id.csv` to `df_categories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read purchase_history.csv; assign the dataframe to `df_baskets`\n",
    "df_baskets = ____\n",
    "\n",
    "# Read item_to_id.csv; assign the dataframe to `df_categories`\n",
    "df_categories = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Inspect the head of df_baskets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the head of df_baskets\n",
    "____.____()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Set `customer_id` as the index of `df_baskets`. Inspect `df_basket` again to make sure that everything went OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `customer_id` as the index of `df_baskets`\n",
    "____.____('____', ____=____, ____=T____rue)\n",
    "\n",
    "# Inspect the head of `df_baskets` again ...\n",
    "____.____()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Inspect the head of `df_categories`. Inspect the names of `df_categories` columns; did you notice anything about these names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the head of `df_categories`\n",
    "____.____()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make column names lowercase\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 'item_id' as index\n",
    "____.____('____', ____=____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Missing Values\n",
    "\n",
    "**Task**: Check the number of missing values per column for each of the two dataframes. Devise a method to treat these missing values in case they are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the number of missing values per column for `df_baskets` \n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the number of missing values per column for `df_categories` \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Construct a Customer-Item Dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now construct a customer-item dataframe where each row has the the customer ids as an index and the item ids as columns. Each cell in this table should contain the number of purchases made by a certain customer for a certain item.\n",
    "\n",
    "You will first start by building a transaction-items dataframe where each column corresponds to an item id.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- Write a function called `customer_item_count` that takes a `pandas.Series` `s` as an argument:\n",
    "    - Raise a `ValueError` if the `basket` column is not contained in `s`.\n",
    "    - Manipulate the `basket` column in `s` to determine the list of items bought by a customer. Assign the result to `items`.\n",
    "    - Return a `pandas.Series` object whose index corresponds to the id of an item and whose values corresponds to the number of purchases of that item.  You can use the `Counter` function from the `collections` module to facilitate your task.\n",
    "    \n",
    "    \n",
    "- Apply the function `customer_item_counts` to the `df_baskets` dataframe using the `.apply()` method:\n",
    "    - Make sure to apply this function along the rows of `df_basket` by setting the `axis` argument correctly. \n",
    "    - Finally chain the result with the method `.fillna()` to fill missing values with zeros. Assign the final dataframe to `df_customer_transaction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_item_count(s):\n",
    "    '''\n",
    "    This function takes a pandas.Series s corresponding to a row in df_baskets\n",
    "    and returns a pandas.Series whose index corresponds to the id of an item\n",
    "    and whose values corresponds to the number of purchases of a particular\n",
    "    item.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    s: pandas Series\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    pandas Series    \n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError if the 'basket' column is not contained in s\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Make sure that 'basket` is contained in s\n",
    "    if ____ not in ____:\n",
    "        raise ____(\"____\")\n",
    "    \n",
    "    # Compute the list of items in s['basket']\n",
    "    items =  ____\n",
    "    \n",
    "    # Return a pd.Series of whose index corresponds to item ids and\n",
    "    # whose values corresponds to counts of items\n",
    "    return ____\n",
    "\n",
    "#Transform df_baskets into a tidy form\n",
    "df_customer_transaction = ____.____(____,____=____).____(____, ____='____')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are done from constructing a transaction-item dataframe, it's time to construct a customer-item dataframe that takes into account all historical purchases made by a customer.\n",
    "\n",
    "**Task**: \n",
    "\n",
    "Manipulate `df_customer_transaction` to obtain a customer-item dataframe in which the index corresponds to the id of a customer and where the cells contain the total number of purchases of an item. Assign the result to `df_customer_items`.\n",
    "\n",
    "*Hint*: You can use the `groupby()` method to achieve this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by customer_id\n",
    "df_customer_items = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "Print out a random sample of `df_customer_items` consisting of 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out a random sample consisting of 10 records\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcast the columns to integer\n",
    "for col in df_customer_items.columns:\n",
    "    df_customer_items[col] = pd.to_numeric(df_customer_items[col], downcast='integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Most purchased item\n",
    "\n",
    "The owner of the store wants to know which items in the transcations database was purchased the most. For this purpose, you will analyze the data to find the most purchased item.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- Manipulate `df_customer_items` to find the total count of purchases of the different items. Assign the result to `item_purchases`.\n",
    "- Use `item_purchases` to find the name and the id of the most purchased item. Also find the total number of times this item was purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 'item_purchases'\n",
    "item_purchases = ____\n",
    "\n",
    "# Find the id of the most purchased item\n",
    "max_purchased_item_id = ____\n",
    "\n",
    "# Find the name of the most purchased item\n",
    "max_purchased_item_name = ____\n",
    "\n",
    "# Find the number of times the most purchased item appeared\n",
    "max_purchased_item_count = ____\n",
    "\n",
    "# print the result\n",
    "print(\"The most purchased item was {0}, it has the item id {1} and was purchased {2} times.\".format(\n",
    "                  max_purchased_item_name, max_purchased_item_id, max_purchased_item_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Customer who Purchased the Greatest Number of Items\n",
    "\n",
    "The store's owner wants to reward the customer who purchased the greatest number of items. Your task here is to identify this customer's id and determine the total number of items she/he purchased throughout the studied period.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- Manipulate `df_customer_items` to find the total count of purchases for all customers. Assign the result to `customer_purchases`.\n",
    "- Use `item_purchases` to find the id of the customer who purchased the greatest number of items. Also find the total number of items bought by this customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum over columns to find the # of purchases of each customer\n",
    "customer_purchases = ____\n",
    "\n",
    "# Find the id of the customer who purchased the maximum number of items\n",
    "most_purchasing_customer_id = ____\n",
    "\n",
    "# Find the total number of items purchased by the most purchasing customer\n",
    "tot_items_purchased = ____\n",
    "\n",
    "print(\"The customer who purchased the greatest number of items has the id {0},\"\n",
    "      \" and has purchased {1} items.\".format(most_purchasing_customer_id, tot_items_purchased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Distribution of basket size\n",
    "\n",
    "The store's owner wants to get an idea of the distribution of the customers' basket size and to visualize how it looks like. \n",
    "\n",
    "You will help the owner in this task by using the awesome `seaborn` library.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "- Use `sns.distplot()` to plot the distribution of basket size.\n",
    "- Also plot vertical lines indicating the median and the average basket size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,8))\n",
    "\n",
    "# Plot the distribution of basket size\n",
    "____\n",
    "____\n",
    "____\n",
    "ax.set_title(\"Distribution of basket size\", fontsize=15)\n",
    "ax.set(xlabel='Basket Size')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Most purchasing customer by item\n",
    "\n",
    "For each item, find the id of the customer who baught that item the most. You should include each item's name and id.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- Use `df_customer_items` to construct a dataframe `max_purchase_by_item`  that shows for each item_id, the maximum number of purchases by a single customer as well as the id of the customer. \n",
    "- Set the name of `max_purchase_by_item`'s index to `item_id`.\n",
    "- Join `max_purchase_by_item` with `df_categories` using `max_purchase_by_item`'s `.join()` method. Assign the result to `max_purchase_by_item`.\n",
    "- Display `max_purchase_by_item` and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 'max_purchase_by_item'\n",
    "max_purchase_by_item = ____\n",
    "    \n",
    "# Set the name of 'max_purchase_by_item' to 'item_id'    \n",
    "____ = '____'\n",
    "\n",
    "# Join max_purchase_by_item to df_categories\n",
    "max_purchase_by_item = ____.____(____)\n",
    "\n",
    "# Display max_purchase_by_item\n",
    "display(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Items similarity matrix\n",
    "\n",
    "In this section, you will build a matrix that represents the similarity between items based on the past purchase behavior of customers. You will first have to normalize each column in the `df_customer_items` dataframe to bring all the 'item' features on a similar scale. \n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- Import `normalize` from `sklearn.preprocessing`.\n",
    "- Normalize the columns of `df_customer_item` using the `normalize` function. Assign the result to `customer_item_matrix`.\n",
    "- Find a way to construct a matrix that measures the similarity between items. Assign the result to `item_similarity`.\n",
    "- Transform `item_similarity` to a dataframe whose indices and columns correpond to the ids of items. Assign the result to `df_item_similarity`.\n",
    "- Inspect the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import normalize from sklearn.preprocessing\n",
    "from ____.____ import ____\n",
    "\n",
    "# Normalize df_customer_items by row\n",
    "customer_item_matrix = ____(____, axis=0)\n",
    "\n",
    "# Find the matrix of item similarity\n",
    "item_similarity = ____\n",
    "\n",
    "# Transform the similarity matrix into a dataframe\n",
    "df_item_similarity = pd.DataFrame(____, \n",
    "                                  index= ____,\n",
    "                                  columns= ____)\n",
    "\n",
    "# Delete item_similarity to save space\n",
    "del item_similarity\n",
    "\n",
    "# Inspect df_item_similarity\n",
    "df_item_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Machine Learning\n",
    "\n",
    "Wow! That was pretty lengthy. You went all the way from having a table that contained individual transactions to a dataframe representing the similarity between the items present in the transactional database.\n",
    "\n",
    "Now it's time to apply some machine learning techniques to cluster similar items together!\n",
    "\n",
    "Let's first start by performing some basic imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score # Model evaluation\n",
    "from sklearn.cluster import KMeans # Clustering\n",
    "from sklearn.decomposition import PCA # Dimensionality reduction\n",
    "from sklearn.pipeline import make_pipeline # Import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler # StandardScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Dimensionality Reduction with PCA\n",
    "\n",
    "Before feeding `df_item_similarity` to a clustering model, we shall reduce its dimensionality using Principal Component Analysis (PCA). The PCA algorithm diagonalizes the covariance matrix of `df_item_similarity` and finds decorrelated components as new features. These new components are linear combinations of the original components.\n",
    "\n",
    "In order to reduce the dimensionality of the dataset, we can use the variance explained ratio to capture an arbitratry percentage of `df_item_similarity`'s variance.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- Set the variable `percentage_variance` to 0.95 in order to capture 95% of the variance in the PCA model.\n",
    "- Instantiate a `PCA` object; set the parameter `n_components` to `percentage_variance`. Assign this object to `pca`.\n",
    "- Fit `pca` to `df_item_similarity`.\n",
    "- Use `pca` to transform `df_item_similarity` to `X_transformed`.\n",
    "- Convert `X_transformed` into a dataframe whose index corresponds to the index of `df_item_similarity` and whose columns are denoted `PCi` where `i` is an integer that runs from 1 to the reduced total number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline: - Standardize the data, -then reduce dimensionality to capture 95% of the variance\n",
    "percentage_variance = ____\n",
    "pca = ____(____=____)\n",
    "\n",
    "# Find the PCA transform of df_item_similarity\n",
    "X_transformed = ____.____(____)\n",
    "\n",
    "# Convert 'X_transformed' into a dataframe\n",
    "X_transformed = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the cumulative variance explained ratio\n",
    "\n",
    "The code chunk below produces a barplot of the cumulative variance explained ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative variance explained ratio\n",
    "\n",
    "# Determine the number of columns to take into account\n",
    "no_cols = ____\n",
    "\n",
    "# create a pandas.Series of the cumulative explained variance ratio\n",
    "# Set the index as the column names and sort the values of the series\n",
    "cum_explained_variance = pd.Series(____,\n",
    "                                   index = ____).sort_values(ascending=____)\n",
    "\n",
    "# Draw a barplot of 'cum_explained_variance'\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "____.plot(____='____', ax=ax, color='lightblue')\n",
    "\n",
    "ax.grid(ls='--')\n",
    "ax.axhline(y=percentage_variance, color='red', ls='--')\n",
    "plt.suptitle('Cumulative explained variance ratio', fontsize=20)\n",
    "plt.xticks(range(1,no_cols), rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture 95% of the variance, PCA reduced the dimensionality of the dataset from 48 to 40. Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to perform clustering using the K-Means algorithm. Since you do not know apriori the optimal number of clusters, you shall vary this number and determine the inertia as well as the average silhouette score.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- Define a list named `clusters` that contains the number of clusters ranging from 2 to 30.\n",
    "- Define two empty lists named `silhouette_scores` and `inertias`.\n",
    "- Iterate over the `clusters` list using `n_clusters` as a for-loop variable:\n",
    "\n",
    "    - Instantiate a `KMeans` class with `n_clusters`; assign this instance to `kmeans`.\n",
    "    - Fit `kmeans` on `X_transformed`.\n",
    "    - Precict the labels of `X_transformed`, assign the result to `predicted_labels`.\n",
    "    - Append the inertia of the clustering model to the list `inertias`.\n",
    "    - Append the silhouette score to the list `silhouette_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering for k ranging from 2 to 30 on transformed data\n",
    "\n",
    "# Define the list 'clusters'\n",
    "clusters = ____\n",
    "\n",
    "# Defube silhouette_scores and inertias\n",
    "silhouette_scores = []\n",
    "inertias = []\n",
    "\n",
    "for ____ in ____:\n",
    "    \n",
    "    # Instantiate a KMeans object with i clusters, assign it to kmeans\n",
    "    kmeans = ____\n",
    "    \n",
    "    # Fit kmeans to X_transformed\n",
    "    ____\n",
    "    \n",
    "    # Predict labels\n",
    "    predicted_labels = ____\n",
    "\n",
    "    # Determine inertia\n",
    "    ____\n",
    "    \n",
    "    # Determine silhouette score\n",
    "    score_silhouette = ____\n",
    "    silhouette_scores.append(____)\n",
    "      \n",
    "    print('Finished clustering with {0} clusters'.format(n_clusters))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Inertia and Silhouette Score\n",
    "\n",
    "Execute the following chunk of code to plot the inertia diagram as well as the silhouette plot. Review the plots to estimate the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_scores = pd.Series(inertias, index=clusters)\n",
    "silhouette_scores = pd.Series(silhouette_scores, index = clusters)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "inertia_scores.plot(ax=ax[0], marker='o')\n",
    "ax[0].set_xlabel('Number of Clusters')\n",
    "ax[0].set_ylabel('Inertia')\n",
    "ax[0].grid(ls='--',alpha=0.5)\n",
    "\n",
    "silhouette_scores.plot(ax=ax[1], marker='o')\n",
    "ax[1].set_xlabel('Number of Clusters')\n",
    "ax[1].set_ylabel('Silhouette Score')\n",
    "ax[1].grid(ls='--',alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Silhouette Plot\n",
    "\n",
    "You will now draw a silhouette plot of the clustering model with the optimal number of clusters. For your convenience, the function `silhouette_plot` is written in the code chunk below. You can use it to obtain a silhouette plot of your clustering model.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "- Use the graphs above to select the optimal number of clusters `n_optimal_clusters`. \n",
    "- Instantiate a `KMeans()` class with `n_clusters` set to `n_optimal_clusters`; assign the result to `kmeans`.\n",
    "- Fit `kmeans` on `X_transformed`.\n",
    "- Call `silhouette_plot` and pass `kmeans` and `X_transformed` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "def silhouette_plot(model, X):\n",
    "    '''\n",
    "    This function draws a silhouette plot of a clustering model given the features \n",
    "    matrix X\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    model: sklean.cluster class\n",
    "        The clustering model to visualize.\n",
    "    \n",
    "    X: A pandas.DataFrame corresponding\n",
    "        The numpy.array of pandas.DataFrame corresponding to the features matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plot: matplotlib figure object\n",
    "        A plot showing a silhouette plot of the clusters deduced by the model on X.\n",
    "   \n",
    "    '''\n",
    "    \n",
    "    y_km = model.predict(X)\n",
    "    cluster_labels = np.unique(y_km)\n",
    "\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    \n",
    "    silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')\n",
    "    \n",
    "    y_ax_lower, y_ax_upper = 0, 0\n",
    "    yticks = []\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    for i, c in enumerate(cluster_labels):\n",
    "        c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "        c_silhouette_vals.sort()\n",
    "        y_ax_upper += len(c_silhouette_vals)\n",
    "        color = cm.jet(float(i) / n_clusters)\n",
    "        plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n",
    "                 edgecolor='none', color=color)\n",
    "\n",
    "        yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "        y_ax_lower += len(c_silhouette_vals)\n",
    "\n",
    "    silhouette_avg = silhouette_score(X_transformed, y_km, metric='euclidean') \n",
    "    plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "    plt.yticks(yticks, cluster_labels + 1)\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.xlabel('Silhouette coefficient')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('K-means with {0} clusters'.format(len(cluster_labels)), fontsize=20)\n",
    "    plt.xlim(-0.1,0.35)\n",
    "    plt.show()\n",
    "##########################################################################################\n",
    "\n",
    "# Set n_optimal_clusters\n",
    "n_optimal_clusters = ____\n",
    "\n",
    "# Instantiate a KMeans \n",
    "kmeans = ____(____=____)\n",
    "\n",
    "# Fit kmeans to X_transformed\n",
    "____\n",
    "\n",
    "# Call silhouette plot\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualizing the Clusters in 2D\n",
    "\n",
    "Finally, now that you selected the optimal model, you can visualize the shape of the obtained clusters in 2D. \n",
    "\n",
    "You'll also examine the items forming each of the obtained clusters.\n",
    "\n",
    "For your convenience, the function `show_clusters` is written in the code chunk below. You can call this function to produce a figure showing a projection of the data along the first two principal components. The function also outputs the items contained in each cluster. \n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "- Call the function `show_clusters` and pass the trained `kmeans` object as well as the dataframe `X_transformed` as arguments.\n",
    "- Inspect the output and analyze whether the results make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "def show_clusters(model, X):\n",
    "    \"\"\"\n",
    "    Display a figure that shows the projection of X along the first 2 principal components\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: a trained sklearn.cluster class\n",
    "    \n",
    "    X: a pandas.DataFrame object corresponding to the PCA-reduced features matrix,\n",
    "       the column names should be 'PCi' with i running from 1 to the maximum number\n",
    "       of components.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plot: matplotlib figure object\n",
    "    \n",
    "    \"\"\"\n",
    "    labels = model.predict(X)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    colors =  it.cycle ([\"b\",\"g\",\"r\",\"c\",\"m\",\"y\",\"k\"])\n",
    "\n",
    "    groups = X.groupby(labels)\n",
    "    \n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "    ax = fig.add_subplot(111,)\n",
    "    for (label,group) in groups:        \n",
    "#         ax.scatter(group['PC1'],group['PC2'],group['PC3'],c=next(colors),label = label, )\n",
    "        ax.scatter(group['PC1'],group['PC2'],c=next(colors),label = label, )\n",
    "\n",
    "        print(\"\\n*********** Cluster [{}] ***********\\n\".format(label+1))\n",
    "        names = df_categories.loc[ df_categories.index.isin(group.index), 'item_name']\n",
    "        for index, name in enumerate(names):\n",
    "            print(\"\\t{} {}\".format(index+1,name))\n",
    "\n",
    "    # annotate\n",
    "    for itemid in X.index:\n",
    "        x = X.loc[itemid,\"PC1\"]\n",
    "        y = X.loc[itemid,\"PC2\"]\n",
    "#         z = X.loc[itemid, \"PC3\"]\n",
    "        name = df_categories.loc[df_categories.index == itemid,\"item_name\"].values[0]\n",
    "#         ax.text(x,y,z,name)\n",
    "        ax.text(x, y ,name)\n",
    "        \n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "################################################################################################\n",
    "    \n",
    "# Call show_clusters and pass X_transformed and labels as arguments\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for attending the workshop!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
